---
title: Vision Transformer(ViT)论文笔记
date: 2024-07-01 19:26:00 +0800
categories: [Blog, Computer Vision]
tags: [computer vision, deep learning, paper note]
math: true
---

NLP里的一个单词👉CV里的一个Patch（16$\times$​16，通过一个FC得到embedding）。Patch数相当于一句话里的单词数。

## 归纳偏置inductive biases

相比resnet来说ViT缺少一些归纳偏置，这导致后者比前者在一些中小的数据集上准确率低几个点。ib指的是一些先验知识。

CNN中的两个ib：locality（假设图片上相邻区域会有相邻的特征），translation equivariance（平移等变性，即先卷积再平移或先平移再卷积是一样的）。

与其他在CV中使用自注意力机制的工作不同，ViT除了位置编码和抽patch以外没有使用其他domain knowledge，简单地使用NLP的transformer。

鉴于CNN比较data efficient（在中小数据集上效果较好），ViT的全局建模能力较好（需要更大数据集），可以考虑使用混合网络（文中指的是使用CNN在一开始进行特征提取得到16$\times$16的特征图，把它作为256个patch再输入全连接层得到patch embedding，后续使用ViT）。

## 使用BERT的class token还是全局平均池化（GAP）？

结果上来看差不多，但是需要调学习率。

*BERT的class token是什么？得看BERT，以后。

GAP：直观上是把特征图拉成一个向量。用于替代全连接层时，将最后一层的特征图每个通道分别取平均，拼成一个长度为C的向量送入后续计算（如分类用的激活函数）。

## *Transformer中为什么只用一套MLP权重？

首先，embedding经过transformer层得到的特征向量是同时包含**原单词信息**和**原单词和其他单词的关系信息**的，因为其生成方法如下图：

![self_attention_1](https://imagebed-1327657732.cos.ap-guangzhou.myqcloud.com/img/self_attention_1.png)

可见$ b^1 $对应单词$x^1$生成的embedding：$a^1$，是通过$q^1$（query）和其余各单词的key求相似度得到的权重并对所有单词的value加权求和，故既含原单词信息又含和其他单词的关系信息。

而所谓位置逐元素前馈网络（Position-wise Feed-Forward Networks）的目的是将特征投影到一个更有效的语义空间，实际上是一种MLP。这里的position-wise指的是对序列中的每个位置（即单词）分别使用MLP。这个网络如下：


$$
FFN(x)=ReLU(xW_1+b_1)W_2+b_2
$$


其中第一个线性层的$W_1$​将原特征向量的长度从512投影为2048，从而扩展每个位置的表示，为学习更复杂的特征提供可能性。激活函数帮助模型学习更复杂的非线性特征，然后用第二个线性层将每个位置的表示压缩回原始维度512，这是因为有残差连接。

原文提到：

*While the linear transformations are the same across different positions, they use different parameters from layer to layer*.

也就是说这个FFN层在一个layer里共享参数，即对同一层里所有位置的特征图进行的是一样的操作。对此李沐老师给出的解释是：在attention层已经把序列信息汇聚完成，所以MLP可以分开在每个位置进行。

![why_MLP_share_params_LiMu](https://imagebed-1327657732.cos.ap-guangzhou.myqcloud.com/img/why_MLP_share_params_LiMu.png)

_李沐老师画的图（BV1pu411o7BE）_

## *为什么Multi-head有效？（待写）

